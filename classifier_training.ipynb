{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Subset, DataLoader \n",
    "from torch.backends import cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.models import alexnet\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.models import resnet18, resnet50\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import args\n",
    "import utils\n",
    "from networks import ImageClassifer\n",
    "\n",
    "if not os.path.exists(args.CLASSIFIER_WEIGHT_DIR):\n",
    "  os.mkdir(args.CLASSIFIER_WEIGHT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root=args.CLASSIFIER_TRAINING_DIR, transform=utils.content_img_transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "\n",
    "NUM_CLASSES = len(train_dataset.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ImageClassifer(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 1e-3            # The initial Learning Rate 1e-2\n",
    "MOMENTUM = 0.9       # Hyperparameter for SGD, keep this at 0.9 when using SGD\n",
    "WEIGHT_DECAY = 5e-5  # Regularization, you can keep this at the default\n",
    "\n",
    "NUM_EPOCHS = 30      # Total number of training epochs (iterations over dataset)\n",
    "STEP_SIZE = 20       # How many epochs before decreasing learning rate (if using a step-down policy)\n",
    "GAMMA = 0.1          # Multiplicative factor for learning rate step-down\n",
    "\n",
    "LOG_FREQUENCY = 10\n",
    "FREEZE = 'conv_layers'   # Available choice: 'no_freezing', 'conv_layers', 'fc_layers'\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Choose parameters to optimize and which one to freeze\n",
    "if (FREEZE == 'no_freezing'):\n",
    "  parameters_to_optimize = net.vgg16.parameters() # In this case we optimize over all the parameters of AlexNet\n",
    "elif (FREEZE == 'conv_layers'):\n",
    "  parameters_to_optimize = net.vgg16.classifier.parameters() # Updates only fully-connected layers (no conv)\n",
    "elif (FREEZE == 'fc_layers'):\n",
    "  parameters_to_optimize = net.vgg16.features.parameters() # Updates only conv layers (no fc)\n",
    "else :\n",
    "  raise (ValueError(f\"Error Freezing layers (FREEZE = {FREEZE}) \\n Possible values are: 'no_freezing', 'conv_layers', 'fc_layers' \"))\n",
    "\n",
    "optimizer = optim.SGD(parameters_to_optimize, lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "EVAL_ACCURACY_ON_TRAINING = True\n",
    "criterion_val = nn.CrossEntropyLoss(reduction='sum') # for evaluation I don't want to avg over every minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Hyperparameters: \n",
      "N_EPOCHS: 30\n",
      "STEP_SIZE: 20\n",
      "Optimizer: \n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 5e-05\n",
      ")\n",
      "Starting epoch 1/30, LR = [0.001]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/machsmt/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Loss 4.642947196960449\n"
     ]
    }
   ],
   "source": [
    "print(\"Current Hyperparameters: \")\n",
    "print(f\"N_EPOCHS: {NUM_EPOCHS}\")\n",
    "print(f\"STEP_SIZE: {scheduler.step_size}\")\n",
    "print(f\"Optimizer: \\n{optimizer}\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# By default, everything is loaded to cpu\n",
    "net = net.to(device) # bring the network to GPU if DEVICE is cuda\n",
    "cudnn.benchmark # Calling this optimizes runtime\n",
    "\n",
    "# save best config\n",
    "best_net = 0\n",
    "best_epoch = 0\n",
    "best_train_acc = 0.0\n",
    "\n",
    "# save accuracy and loss\n",
    "train_accuracies = []\n",
    "train_losses = []\n",
    "\n",
    "current_step = 0\n",
    "\n",
    "# Start iterating over the epochs\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  print(f\"Starting epoch {epoch+1}/{NUM_EPOCHS}, LR = {scheduler.get_last_lr()}\")\n",
    "  \n",
    "  net.train() # Sets module in training mode\n",
    "\n",
    "  running_corrects_train = 0\n",
    "  running_loss_train = 0.0\n",
    "\n",
    "  # Iterate over the training dataset\n",
    "  for images, labels in train_dataloader:\n",
    "\n",
    "    # Bring data over the device of choice\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # PyTorch, by default, accumulates gradients after each backward pass\n",
    "    # We need to manually set the gradients to zero before starting a new iteration\n",
    "    optimizer.zero_grad() # Zero-ing the gradients\n",
    "  \t\n",
    "    with torch.set_grad_enabled(True):\n",
    "\n",
    "      # Forward pass to the network\n",
    "      outputs_train = net(images)\n",
    "\n",
    "      _, preds = torch.max(outputs_train, 1)\n",
    "\n",
    "      # Compute loss based on output and ground truth\n",
    "      loss = criterion(outputs_train, labels)\n",
    "\n",
    "      # Log loss\n",
    "      if current_step % LOG_FREQUENCY == 0:\n",
    "        print('Step {}, Loss {}'.format(current_step, loss.item()))\n",
    "\n",
    "      # Compute gradients for each layer and update weights\n",
    "      loss.backward()  # backward pass: computes gradients\n",
    "      optimizer.step() # update weights based on accumulated gradients\n",
    "\n",
    "    current_step += 1\n",
    "\n",
    "  # store loss and accuracy values\n",
    "  running_corrects_train += torch.sum(preds == labels.data).data.item() \n",
    "  running_loss_train += loss.item() * images.size(0)\n",
    "  \n",
    "  train_acc = running_corrects_train / float(len(train_dataset))\n",
    "  train_loss = running_loss_train / float(len(train_dataset))\n",
    "\n",
    "  train_accuracies.append(train_acc)\n",
    "  train_losses.append(train_loss) # loss computed as the average on mini-batches\n",
    "  #train_loss.append(loss.item()) # loss computed only on the last batch\n",
    "\n",
    "  ### END TRAINING PHASE OF AN EPOCH\n",
    "\n",
    "  # Check if the current epoch val accuracy is better than the best found until now\n",
    "  if (train_acc >= best_train_acc) :\n",
    "    print(f\"\\nSave model: {best_epoch+1}\\n{best_train_acc:.4f} (Training Accuracy)\\n\")\n",
    "    print(f\"> In {(time.time()-start)/60:.2f} minutes\") \n",
    "    best_train_acc = train_acc\n",
    "    best_epoch = epoch\n",
    "    best_net = copy.deepcopy(net) # deep copy the model\n",
    "    # save the model\n",
    "    torch.save(best_net.state_dict(), args.CLASSIFIER_MODEL_PATH)\n",
    "  \n",
    "  # Step the scheduler\n",
    "  scheduler.step() \n",
    "\n",
    "print(f\"\\nBest epoch: {best_epoch+1}\\n{best_train_acc:.4f} (Training Accuracy)\\n\")\n",
    "print(f\"> In {(time.time()-start)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machsmt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
